# To Do

List of stuff I don't feel like filing an issue for (yet).


## To Do
- Schema for Users DB
- Mapping for users from users DB to other DBs/permissions mgmt
- searx https://github.com/Jay4242/llm-websearch/blob/main/llm-websearch.bash
- Blogpost
- https://adamj.eu/tech/2024/12/30/python-temporary-files-directories-unittest/
- Documentation
- Update README
- Update CONTRIBUTING 
- Scraping Pipeline-related
    - Fully test + Write tests for cookie cloner script
    - Update Scraping pipeline to use cookie cloner script
    - Add option to Scraping page to use cookie cloner script
- TTS/STT-related
    - https://github.com/rmusser01/tldw/issues/8
    - Add page for 
- Building Infra
      - https://notes.billmill.org/blog/2024/06/Serving_a_billion_web_requests_with_boring_code.html
- API
      - https://quart.palletsprojects.com/en/latest/tutorials/api_tutorial.html
- Add global max tokens value to config + Per API max tokens value
- Add global System prompts for Chat API calls
- Add global System prompts for Summarization API calls
- ERROR:root:Error in perform_full_text_search (Media DB): 'NoneType' object has no attribute 'split'

- 
- PDF Parsing
    - marker https://github.com/VikParuchuri/marker
    https://github.com/pdf2htmlEX/pdf2htmlEX
    https://camelot-py.readthedocs.io/en/master/
    https://github.com/kermitt2/grobid
    https://arxiv.org/html/2410.09871v1#S6
    https://github.com/Filimoa/open-parse/
    https://github.com/nlmatics/nlm-ingestor
    https://github.com/conjuncts/gmft
    https://github.com/xavctn/img2table
    Benchmarks
        https://github.com/py-pdf/benchmarks
    Pipeline
        https://ai.gopubby.com/demystifying-pdf-parsing-02-pipeline-based-method-82619dbcbddf
        https://ai.gopubby.com/demystifying-pdf-parsing-04-ocr-free-large-multimodal-model-based-method-0fdab50db048
        https://pub.towardsai.net/demystifying-pdf-parsing-05-unifying-separate-tasks-into-a-small-model-d3739db021f7
        https://ai.gopubby.com/demystifying-pdf-parsing-06-representative-industry-solutions-5d4a1cfe311b

- Prompts
  - https://github.com/ProfSynapse/Synapse_CoR


Links to sort
https://github.com/dylanashley/story-distiller
https://ieeexplore.ieee.org/document/10734853
https://towardsdatascience.com/agentic-chunking-for-rags-091beccd94b1
https://www.youtube.com/watch?v=NVp9jiMDdXc
https://github.com/n8n-io/n8n-docs
https://huggingface.co/NeuML/pubmedbert-base-embeddings
https://huggingface.co/datasets/qiaojin/PubMedQA
https://huggingface.co/datasets/armanc/scientific_papers
https://www.notion.com/blog/how-we-sped-up-notion-in-the-browser-with-wasm-sqlite
https://alexgarcia.xyz/blog/2024/sqlite-vec-hybrid-search/index.html
https://needle-threading.github.io/
https://vitron-llm.github.io/
https://huggingface.co/papers/2411.04335
https://github.com/severian42/Cascade-of-Semantically-Integrated-Layers
https://github.com/THUDM/LongReward
RAG
https://towardsdatascience.com/improve-your-rag-context-recall-by-40-with-an-adapted-embedding-model-5d4a8f583f32
https://towardsdatascience.com/beyond-na%C3%AFve-rag-advanced-techniques-for-building-smarter-and-reliable-ai-systems-c4fbcf8718b8
https://ai.gopubby.com/rag-on-steroids-self-rag-69505f9d8238
https://github.com/dongguanting/DPA-RAG
https://arxiv.org/html/2411.02959v1
https://arxiv.org/pdf/2409.05591


ASR


Hallucinations
https://arxiv.org/abs/2410.22071

Scraping
https://github.com/devflowinc/firecrawl-simple

Prompts
https://colab.research.google.com/drive/1fw7ge47ymnznsz3rWlXVcyPC9PKk6_xH#scrollTo=-9mw9XLfj_vD

Finetuning
https://towardsdatascience.com/i-fine-tuned-the-tiny-llama-3-2-1b-to-replace-gpt-4o-7ce1e5619f3d

Smaller models
https://towardsdatascience.com/leveraging-smaller-llms-for-enhanced-retrieval-augmented-generation-rag-bc320e71223d

Multimodal
https://magazine.sebastianraschka.com/p/understanding-multimodal-llms?utm_source=substack&utm_medium=email
https://arxiv.org/abs/2410.21943

Ranking
https://archive.is/hXXK6

Evals
https://towardsdatascience.com/llm-evaluation-techniques-and-costs-3147840afc53
https://archive.is/IA4UR
https://github.com/huggingface/lighteval
https://github.com/tianyi-lab/BenTo

Prompts
https://huggingface.co/spaces/baconnier/prompt-plus-plus
https://github.com/google-deepmind/opro


https://arxiv.org/pdf/2410.11795
https://dynamic-city.github.io/
https://generative-infinite-game.github.io/
https://arxiv.org/abs/2410.18417
https://arxiv.org/html/2410.18745v1
https://arxiv.org/html/2410.13293v1
https://huggingface.co/papers/2410.13852
https://huggingface.co/papers/2410.09584
https://huggingface.co/papers/2410.10813
https://arxiv.org/pdf/2409.12640
https://huggingface.co/papers/2410.06634
https://huggingface.co/papers/2410.10594
https://github.com/ShayekhBinIslam/openrag
https://huggingface.co/papers/2410.08815
https://huggingface.co/papers/2408.11875
https://huggingface.co/papers/2409.19753
https://arxiv.org/abs/2410.08815
https://arxiv.org/abs/2410.07035
https://arxiv.org/abs/2311.04954
https://lmql.ai/
https://artificialintelligencemadesimple.substack.com/p/how-amazon-is-rethinking-human-evaluation
https://arxiv.org/abs/2410.08037
https://github.com/Rolandjg/skool4free
https://arxiv.org/abs/2410.05229
https://arxiv.org/abs/2410.07176
https://arxiv.org/abs/2410.05983
https://huggingface.co/papers/2410.04199
https://arxiv.org/html/2409.14924v1
https://huggingface.co/papers/2407.13101
https://huggingface.co/spaces/Xenova/the-tokenizer-playground
https://github.com/ml-jku/RA-DT
https://arxiv.org/abs/2410.07071
https://huggingface.co/papers/2410.03017
https://huggingface.co/papers/2409.18943
https://arxiv.org/pdf/2409.16493

```
 would like to access a generated markdown page of the Timeline, FAQ, Study Guide and Briefing doc.  and pin that and be able to send it out to people rather the the full notebook, a '.md' file.

A way to generate that on the fly would be a really on-brand feature. that could promote collaboration and quick prototyping and discussion within the human groups using NotebookLM.
```

``````
Proposal:
I'd like to suggest adding a third button/section, perhaps called "Subscriptions" or "Watchlist".

How it would work:
In this section, users could add links to:

    RSS feeds
    YouTube playlists
    Entire YouTube channels


NotebookLM would then periodically check these sources for new content (new RSS posts, new videos). The "Subscriptions" section would display a list of the latest items from the added feeds/channels. Users could then easily review this list and select (accept or decline) the items they want to import as new sources into their notebook.

Benefits:

    Automation: Makes it easier to track regularly updated sources (like news, industry blogs, video tutorial series).
    Efficiency: Instead of manually searching for and adding each new article/video, we'd have a quick way to select and import relevant updates.
    Up-to-dateness: Helps keep notebooks powered by the latest information from the channels we follow.



# Executive Summary: Phase 2 Plan for Sync System

After the initial local-first SQLite deployment, the next critical steps for scaling are: (1) enforce **server-assigned timestamps** to eliminate clock drift issues, (2) persist **deferred link operations** across sessions for robustness, (3) implement **client acknowledgment tracking and garbage collection** for `sync_log` growth control, (4) prepare for a **Postgres backend** with Row-Level Security for multi-tenant support, and (5) enhance **metrics and observability**.  
**Timing:** These upgrades must begin before expanding beyond ~20 devices or any production multi-user environment to avoid costly retrofitting.

Hereâ€™s a **clean, forward-looking "Phase 2 Growth Plan" memo** you can use internally or forward to leadership:

---

# Phase 2: Growth Plan for Synchronization System  
**(Post-Initial SQLite Deployment)**

## Objective:
Ensure that the synchronization architecture, originally designed for small teams and single-device use cases, can **scale safely and predictably** to multi-user, multi-device, and eventually multi-tenant environments.

---

## Key Focus Areas:

1. **Server-Authoritative Timestamps**  
   Transition from trusting client-generated `last_modified` timestamps to **server-assigned timestamps** during sync ingestion.  
   - Required to eliminate inconsistencies caused by client clock drift.
   - Impacts conflict resolution logic (LWW becomes truly canonical).

2. **Persistent Deferred Queue for Links**  
   Refactor deferred link/unlink operations (`MediaKeywords`) to **persist across sessions**.  
   - Deferred changes must survive application crashes, network failures, or sync retries.
   - Design persistent "deferred queue" table or mechanism.

3. **`sync_log` Garbage Collection Mechanism**  
   Implement **client acknowledgment** tracking and **safe vacuuming** of old `sync_log` entries.
   - Prevent unbounded database growth.
   - Server must track per-client sync state (last change_id or last_modified).
   - Design deletion policies (e.g., "only purge changes acknowledged by 100% of active clients").

4. **Postgres Backend Hardening**  
   Prepare for a Postgres backend with:
   - Equivalent metadata integrity protections (triggers or `CHECK` constraints).
   - **Row Level Security (RLS)** policies for true tenant isolation.
   - Updated `sync_log` ingestion pipelines for multi-tenant awareness.

5. **Security Enhancements (as needed):**
   - Cryptographic signing and verification of sync payloads (if clients become less trusted).
   - Enforce transport-layer encryption (HTTPS/TLS mandatory).
   - Scope-based OAuth authorization if adopting multi-user models.

6. **Enhanced Observability and Metrics:**
   - Integrate full metrics pipeline (e.g., Prometheus, OpenTelemetry).
   - Track key sync metrics:
     - Fetch latency
     - Apply latency
     - Conflict rates
     - Deferred operation rates
     - Client sync error rates

---

## Strategic Timing:

| Milestone | Trigger to Begin Next Phase |
|:--|:--|
| Local-first SQLite sync | âœ…  (Now) |
| Small team (5â€“20 devices) | **Start server timestamp enforcement, deferred persistence** |
| Shared team-wide sync server | **Implement `sync_log` ACK tracking and garbage collection** |
| Multi-team/multi-tenant deployments | **Postgres hardening + OAuth/RLS** |

---
