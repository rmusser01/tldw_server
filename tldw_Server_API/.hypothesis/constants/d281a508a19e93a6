# file: /Users/appledev/Working/tldw_server/tldw_Server_API/app/core/LLM_Calls/LLM_API_Calls_Local.py
# hypothesis_version: 6.135.11

[0.7, 120, 180, 200, 300, 429, 500, 502, 503, 504, 2048, 4096, '/', '127.0.0.1', 'Aphrodite Engine', 'Authorization', 'Content-Type', 'KoboldAI (Native)', 'Llama.cpp', 'Ollama', 'POST', 'TabbyAPI', 'X-Api-Key', '_summary.txt', 'aphrodite', 'aphrodite_api', 'api_ip', 'api_key', 'api_retries', 'api_retry_delay', 'api_timeout', 'api_url', 'application/json', 'assistant', 'choices', 'code', 'content', 'custom_openai_2_api', 'custom_openai_api', 'custom_openai_api_2', 'data: [DONE]\n\n', 'error', 'finish_reason', 'format', 'frequency_penalty', 'http://', 'https://', 'image_url', 'iteration_error', 'json', 'json_object', 'kobold', 'kobold_api', 'llama.cpp', 'llama_api', 'local_llm', 'localhost', 'logit_bias', 'logprobs', 'max_context_length', 'max_length', 'max_tokens', 'maxp', 'message', 'messages', 'min_p', 'minp', 'model', 'n', 'n_predict', 'n_probs', 'none', 'num_predict', 'num_responses', 'ollama', 'ollama_api', 'ooba', 'ooba_api', 'presence_penalty', 'prompt', 'rep_pen', 'response_format', 'results', 'role', 'seed', 'stop', 'stop_sequence', 'stream', 'stream_error', 'streaming', 'system', 'tabby_api', 'tabbyapi', 'temp', 'temperature', 'text', 'tool_choice', 'tools', 'top_k', 'top_logprobs', 'top_p', 'topk', 'true', 'type', 'user', 'user_identifier', 'utf-8', 'v1/chat/completions', 'vLLM', 'vllm', 'vllm_api', 'w']