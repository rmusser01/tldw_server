[Processing]
processing_choice = cuda

[Media-Processing]
# File size limits (in MB)
max_audio_file_size_mb = 500
max_pdf_file_size_mb = 50
max_video_file_size_mb = 1000
max_epub_file_size_mb = 100
max_document_file_size_mb = 50
# Processing timeouts (in seconds)
pdf_conversion_timeout_seconds = 300
audio_processing_timeout_seconds = 600
video_processing_timeout_seconds = 1200
# Archive processing limits
max_archive_internal_files = 100
max_archive_uncompressed_size_mb = 200
# Transcription settings
audio_transcription_buffer_size_mb = 10
# General settings
uuid_generation_length = 8

[Chat-Dictionaries]
enable_chat_dictionaries = True
post_gen_replacement = False
post_gen_replacement_dict = ./Helper_Scripts/Chat_Dictionaries/Post_Gen_Replacements.md
chat_dictionary_chat_prompts = ./Helper_Scripts/Chat_Dictionaries/Chat_Prompts.md
chat_dictionary_RAG_prompts = ./Helper_Scripts/Chat_Dictionaries/RAG_Prompts.md
strategy = character_lore_first
max_tokens = 1000
default_rag_prompt = simplified_promptflow_RAG_system_prompt


[Settings]
chunk_duration = 30
words_per_second = 3
save_character_chats = False
save_rag_chats = False
save_video_transcripts = True

[Auto-Save]
save_character_chats = False
save_rag_chats = False


[Prompts]
prompt_sample = "What is the meaning of life?"
video_summarize_prompt = "Above is the transcript of a video. Please read through the transcript carefully. Identify the main topics that are discussed over the course of the transcript. Then, summarize the key points about each main topic in bullet points. The bullet points should cover the key information conveyed about each topic in the video, but should be much shorter than the full transcript. Please output your bullet point summary inside <bulletpoints> tags. Do not repeat yourself while writing the summary."


[Database]
type = sqlite
sqlite_path = Databases/server_media_summary.db
backup_path = ./tldw_DB_Backups/
#Path to the backup location for the database. If the path does not exist, the backup will not be created.
elasticsearch_host = localhost
elasticsearch_port = 9200
# Additionally you can use elasticsearch as the database type, just replace `sqlite` with `elasticsearch` for `type` and provide the `elasticsearch_host` and `elasticsearch_port` of your configured ES instance.
chroma_db_path = Databases/chroma_db
prompts_db_path = Databases/prompts.db
rag_qa_db_path = Databases/RAG_QA_Chat.db
character_db_path = Databases/chatDB.db


[Chunking]
# Chunking Defaults
`chunking_method` = words
# 'method' Can be 'words' / 'sentences' / 'paragraphs' / 'semantic' / 'tokens'
chunk_max_size = 400
chunk_overlap = 200
adaptive_chunking = false
# Use ntlk+punkt to split text into sentences and then ID average sentence length and set that as the chunk size
chunking_multi_level = false
language = english
#
# Default Chunking Options for each media type
#
# Article Chunking
article_chunking_method = 'words'
article_chunk_max_size = 400
article_chunk_overlap = 200
article_adaptive_chunking = false
article_chunking_multi_level = false
article_language = english
#
# Audio file Chunking
audio_chunking_method = 'words'
audio_chunk_max_size = 400
audio_chunk_overlap = 200
audio_adaptive_chunking = false
audio_chunking_multi_level = false
audio_language = english
#
# Book Chunking
book_chunking_method = 'words'
book_chunk_max_size = 400
book_chunk_overlap = 200
book_adaptive_chunking = false
book_chunking_multi_level = false
book_language = english
#
# Document Chunking
document_chunking_method = 'words'
document_chunk_max_size = 400
document_chunk_overlap = 200
document_adaptive_chunking = false
document_chunking_multi_level = false
document_language = english
#
# Mediawiki Article Chunking
mediawiki_article_chunking_method = 'words'
mediawiki_article_chunk_max_size = 400
mediawiki_article_chunk_overlap = 200
mediawiki_article_adaptive_chunking = false
mediawiki_article_chunking_multi_level = false
mediawiki_article_language = english
#
# Mediawiki Dump Chunking
mediawiki_dump_chunking_method = 'words'
mediawiki_dump_chunk_max_size = 400
mediawiki_dump_chunk_overlap = 200
mediawiki_dump_adaptive_chunking = false
mediawiki_dump_chunking_multi_level = false
mediawiki_dump_language = english
#
# Obsidian Note Chunking
obsidian_note_chunking_method = 'words'
obsidian_note_chunk_max_size = 400
obsidian_note_chunk_overlap = 200
obsidian_note_adaptive_chunking = false
obsidian_note_chunking_multi_level = false
obsidian_note_language = english
#
# Podcast Chunking
podcast_chunking_method = 'words'
podcast_chunk_max_size = 400
podcast_chunk_overlap = 200
podcast_adaptive_chunking = false
podcast_chunking_multi_level = false
podcast_language = english
#
# Text Chunking
text_chunking_method = 'words'
text_chunk_max_size = 400
text_chunk_overlap = 200
text_adaptive_chunking = false
text_chunking_multi_level = false
text_language = english
#
# Video Transcription Chunking
video_chunking_method = 'words'
video_chunk_max_size = 400
video_chunk_overlap = 200
video_adaptive_chunking = false
video_chunking_multi_level = false
video_language = english
chunking_types = 'article', 'audio', 'book', 'document', 'mediawiki_article', 'mediawiki_dump', 'obsidian_note', 'podcast', 'text', 'video'


[Embeddings]
embedding_provider = openai
embedding_model = text-embedding-3-small
onnx_model_path = ./App_Function_Libraries/models/onnx_models/
model_dir = ./App_Function_Libraries/models/embedding_models
embedding_api_url = http://localhost:8080/v1/embeddings
embedding_api_key = your_api_key_here
chunk_size = 400
overlap = 200
# 'embedding_provider' Can be 'openai', 'local', or 'huggingface'
# `embedding_model` Set to the model name you want to use for embeddings. For OpenAI, this can be 'text-embedding-3-small', or 'text-embedding-3-large'.
# huggingface: model = dunzhang/stella_en_400M_v5


[API]
anthropic_api_key = <anthropic_api_key>
anthropic_model = claude-3-5-sonnet-20240620
anthropic_streaming = True
anthropic_temperature = 0.7
anthropic_top_p = 0.95
anthropic_min_p = 0.05
anthropic_max_tokens = 4096
anthropic_api_timeout = 90
anthropic_api_retry = 3
anthropic_api_retry_delay = 1
#
# <cohere_api_key>
cohere_api_key = <cohere_api_key>
cohere_model = command-r-plus
cohere_streaming = True
cohere_temperature = 0.7
cohere_max_tokens = 4096
cohere_api_timeout = 90
cohere_api_retry = 3
cohere_api_retry_delay = 1
#
#
deepseek_api_key = <deepseek_api_key>
# Options: 'deepseek-chat' or 'deepseek-reasoner'
deepseek_model = deepseek-chat
deepseek_streaming = True
deepseek_temperature = 0.7
deepseek_max_tokens = 4096
deepseek_api_timeout = 90
deepseek_api_retry = 3
deepseek_api_retry_delay = 1
#
google_api_key = <google_api_key>
# Available Model Options:
google_model = gemini-2-5-pro
google_streaming = True
google_temperature = 0.7
google_max_tokens = 4096
google_api_timeout = 90
google_api_retry = 3
google_api_retry_delay = 1
#
groq_api_key = <groq_api_key>
groq_model = llama3-70b-8192
groq_streaming = True
groq_temperature = 0.7
groq_max_tokens = 4096
groq_api_timeout = 90
groq_api_retry = 3
groq_api_retry_delay = 1
#
# Set to true if targeting router.huggingface.co
huggingface_use_router_url_format = false
# Default router base
huggingface_router_base_url = https://router.huggingface.co/hf-inference/models
huggingface_api_chat_path = v1/chat/completions
huggingface_api_key = <huggingface_api_key>
huggingface_model = Qwen/Qwen3-235B-A22B
huggingface_streaming = True
huggingface_temperature = 0.7
huggingface_max_tokens = 4096
huggingface_api_timeout = 90
huggingface_api_retry = 3
huggingface_api_retry_delay = 1
#
mistral_api_key = <mistral_api_key>
mistral_model = mistral-large-latest
mistral_streaming = True
mistral_temperature = 0.7
mistral_max_tokens = 4096
mistral__api_timeout = 90
mistral_api_retry = 3
mistral_api_retry_delay = 1
#
openai_api_key = <openai_api_key>
openai_model = gpt-4o
openai_streaming = False
openai_temperature = 0.7
openai_top_p = 0.95
openai_max_tokens = 4096
openai_api_timeout = 90
openai_api_retry = 3
openai_api_retry_delay = 1
model_for_summarization = gpt-4o
#
openrouter_api_key = <openrouter_api_key>
openrouter_model = mistralai/mistral-7b-instruct:free
openrouter_max_tokens = 4096
openrouter_api_timeout = 90
openrouter_api_retry = 3
openrouter_api_retry_delay = 1
#
elevenlabs_api_key = <eleven_labs_api_key>
#
custom_openai_api_key = <custom_openai_api_key>
custom_openai_api_ip = <api_ip_here>
custom_openai_api_model = <model_name_here>
custom_openai_api_streaming = True
custom_openai_api_temperature = 0.7
custom_openai_api_top_p = 0.9
custom_openai_api_min_p = 0.05
custom_openai_api_max_tokens = 4096
custom_openai_api_timeout = 90
custom_openai_api__api_retry = 3
custom_openai_api__api_retry_delay = 1

#
custom_openai2_api_key = <custom_openai_api_key>
custom_openai2_api_ip = <api_ip_here>
custom_openai2_api_model = <model_name_here>
custom_openai2_api_streaming = True
custom_openai2_api_temperature = 0.7
custom_openai2_api_top_p = 0.9
custom_openai2_api_min_p = 0.05
custom_openai2_api_max_tokens = 4096
custom_openai2_api_timeout = 90
custom_openai2_api_api_retry = 3
custom_openai2_api_api_retry_delay = 1

#
default_api = openai
default_api_for_tasks = anthropic

[Local-API]
kobold_api_IP = http://127.0.0.1:5001/api/v1/generate
kobold_openai_api_IP = http://127.0.0.1:5001/v1/chat/completions
kobold_api_key =
kobold_streaming = False
kobold_temperature = 0.7
kobold_top_p = 0.9
kobold_min_p = 0.05
kobold_top_k = 100
kobold_max_tokens = 4096
kobold_api_timeout = 90
kobold_api_retry = 3
kobold_api_retry_delay = 1
#
llama_api_IP = http://127.0.0.1:8080/completion
llama_api_key =
llama_streaming = True
llama_temperature = 0.7
llama_top_p = 0.9
llama_min_p = 0.05
llama_top_k = 100
llama_max_tokens = 4096
llama_api_timeout = 90
llama_api_retry = 3
llama_api_retry_delay = 1
#
ooba_api_key =
ooba_api_IP = http://192.168.2.235:5000/v1/chat/completions
ooba_streaming = False
ooba_temperature = 0.7
ooba_top_p = 0.9
ooba_min_p = 0.05
ooba_top_k = 100
ooba_max_tokens = 4096
ooba_api_timeout = 90
ooba_api_retry = 3
ooba_api_retry_delay = 1
#
tabby_api_IP = http://127.0.0.1:5000/v1/chat/completions
tabby_api_key =
tabby_streaming = False
tabby_temperature = 0.7
tabby_top_k = 100
tabby_max_tokens = 4096
tabby_api_timeout = 90
tabby_api_retry = 3
tabby_api_retry_delay = 1
#
vllm_api_IP = http://127.0.0.1:8000/v1/chat/completions
vllm_model =
vllm_api_key =
vllm_streaming = False
vllm_temperature = 0.7
vllm_top_p = 0.9
vllm_min_p = 0.05
vllm_top_k = 100
vllm_max_tokens = 4096
vllm_api_timeout = 90
vllm_api_retry = 3
vllm_api_retry_delay = 1
#
ollama_api_IP = http://127.0.0.1:11434/v1/chat/completions
ollama_api_key =
ollama_model = llama3
ollama_streaming = False
ollama_temperature = 0.7
ollama_top_p = 0.9
ollama_max_tokens = 4096
ollama_api_timeout = 9009
ollama_api_retry = 3
ollama_api_retry_delay = 1
#
aphrodite_api_IP = http://127.0.0.1:8080/completion
aphrodite_api_key =
aphrodite_streaming = False
aphrodite_temperature = 0.7
aphrodite_top_p = 0.9
aphrodite_min_p = 0.05
aphrodite_model =
aprhodite_max_tokens = 4096
aphrodite_api_timeout = 90
aphrodite_api_retry = 3
aphrodite_api_retry_delay = 1
#
max_tokens = 4096
local_api_timeout = 90
local_api_retries = 3
local_api_retry_delay = 5
streaming = True
temperature = 0.7
top_p = 0.9
min_p = 0.05
# https://artefact2.github.io/llm-sampling/


[TTS-Settings]
# General TTS Settings
# Options: 'cpu', 'cuda'
local_tts_device = cpu
default_tts_provider = kokoro
default_tts_voice = af_bella
default_tts_speed = 1
#
# OpenAI TTS Settings
# available voices are 'alloy`, `echo`, `fable`, `onyx`, `nova`, and `shimmer'
default_openai_tts_voice = shimmer
default_openai_tts_speed = 1
# available models are 'tts-1' or 'tts-1-hd'
default_openai_tts_model = tts-1-hd
default_openai_tts_output_format = mp3
default_openai_tts_streaming = False
#
# ElevenLabs TTS Settings
default_eleven_tts_voice = pNInz6obpgDQGcFmaJgB
default_eleven_tts_model =
default_eleven_tts_language_code =
default_eleven_tts_voice_stability =
default_eleven_tts_voice_similiarity_boost =
default_eleven_tts_voice_style =
default_eleven_tts_voice_use_speaker_boost =
default_eleven_tts_voice_pronunciation_dictionary_locators_dict_id =
default_eleven_tts_voice_pronunciation_dictionary_locators_version_id =
default_eleven_tts_speed = 1
# Output options: 'mp3_22050_32', 'mp3_44100_32', 'mp3_44100_64', 'mp3_44100_96', 'mp3_44100_128', 'mp3_44100_192', 'pcm_16000', 'pcm_22050', 'pcm_24000', 'pcm_44100', 'ulaw_8000'
default_eleven_tts_output_format = mp3_44100_128
# Google TTS Settings
default_google_tts_model = FIXME
default_google_tts_voice = FIXME
default_google_tts_speed = 1
#
# MS Edge TTS Settings
edge_tts_voice = FIXME
#
# GPT-Sovits
#
# AllTalk TTS Settings
default_alltalk_tts_speed = 1.0
default_alltalk_tts_voice = alloy
default_alltalk_tts_model = alltalk
default_alltalk_tts_output_format = mp3
alltalk_api_ip = http://127.0.0.1:7851/v1/audio/speech
#
# Kokoro TTS Settings
kokoro_model_path = ./App_Function_Libraries/models/kokoro_models/
default_kokoro_tts_speed = 1.0
# Options: bella, nicole, sarah, sky, adam, michael, emma, isabella, george, lewis
default_kokoro_tts_voice = sky
# Options: onnx, pht
default_kokoro_tts_model = pht
# Options: wav, mp3
default_kokoro_tts_output_format = wav
#
# Self-Hosted OpenAI API Settings
default_custom_openai_tts_voice = shimmer
default_custom_openai_tts_speed = 1
default_custom_openai_tts_model = tts-1-hd
default_custom_openai_tts_output_format = mp3
default_custom_openai_api_ip = http://
default_custom_openai_api_streaming = True


[Search-Engines]
# Search Defaults
search_provider_default = google
search_language_query = en
search_language_results = en
search_language_analysis = en
search_default_max_queries = 10
search_enable_subquery = True
search_enable_subquery_count_max = 5
search_result_rerank = True
search_result_max = 15
search_result_max_per_query = 10
search_result_blacklist = []
search_result_display_type = list
search_result_display_metadata = False
search_result_save_to_db = True
# How you want the results to be written, think 'style' or voice
search_result_analysis_tone =
relevance_analysis_llm = openai
final_answer_llm = openai
#### Search Engines #####
# Baidu
search_engine_api_key_baidu = 1e1b1b1b1b1b1b1b1
#
# Bing
search_engine_api_url_bing = https://api.bing.microsoft.com/v7.0/search
search_engine_api_key_bing = <bing_api_key>
search_engine_country_code_bing = en
#
# Brave
search_engine_api_key_brave_regular = <brave_api_key>
search_engine_api_key_brave_ai = <brave_ai_api_key>
search_engine_country_code_brave = US
#
# DuckDuckGo
#
# Google
search_engine_api_url_google = https://www.googleapis.com/customsearch/v1?
search_engine_api_key_google = <google_api_key>
search_engine_id_google = <google_search_engine_id>
# 0 = Enable / 1 = Disabled
enable_traditional_chinese = 0
# Restricts search results to documents originating in a particular country.
limit_google_search_to_country = False
google_search_country_code = US
google_filter_setting = 1
google_user_geolocation = US
google_ui_language = en
google_limit_search_results_to_language =
google_default_search_results =
google_safe_search = "active"
google_enable_site_search =
google_site_search_include =
google_site_search_exclude =
# https://developers.google.com/custom-search/docs/structured_search#sort-by-attribute
google_sort_results_by =
#
# Kagi
search_engine_api_key_kagi = <kagi_api_key>>
# SearX
search_engine_searx_api = https://search.rhscz.eu/
# Serper
# Tavily
search_engine_api_key_tavily = <tavily_api_key>
# Yandex
search_engine_api_key_yandex = 1e1b1b1b1b1b1b1b1
search_engine_id_yandex = 1e1b1b1b1b1b1b1b1

[Web-Scraper]
web_scraper_api_key =
web_scraper_api_url =
web_scraper_api_timeout = 90
web_scraper_api_retry = 3
web_scraper_api_retry_delay = 1
web_scraper_retry_count = 3
web_scraper_stealth_playwright = False

[Logging]
log_level = INFO
log_file = ./Logs/tldw_app_logs.json
log_metrics_file = ./Logs/tldw_metrics_logs.json
#os.getenv("tldw_LOG_FILE_PATH", "tldw_app_logs.json")
max_bytes =
#int(os.getenv("tldw_LOG_MAX_BYTES", 10 * 1024 * 1024))  # 10 MB
backup_count = 5
#int(os.getenv("tldw_LOG_BACKUP_COUNT", 5))


#[Comments]
#OpenAI Models:
#    gpt-4o
#    gpt-4o-2024-08-06
#    gpt-4o-mini
#    o1-preview
#    o1-mini
#    text-embedding-3-large
#    text-embedding-3-small
#
#Anthropic Models:
#    claude-3-5-sonnet-20241022
#    claude-3-5-sonnet-20240620
#    claude-3-5-haiku-20241022
#    claude-3-opus-20240229
#
#Cohere Models:
#    command-r-plus-08-2024
#    command-r-plus-04-2024
#    command-r-08-2024
#    command-r-03-2024
#
#DeepSeek Models:
#    deepseek-chat
#
#Groq Models:
#    f
#Mistral Models:
#    mistral-large-latest
#    open-mistral-nemo
#    codestral-latest
#    mistral-embed
#    open-mistral-7b
#    open-mixtral-8x7b
#    open-mixtral-8x22b
#    open-codestral-mamba
# Google's Models (11/15/2024): https://ai.google.dev/gemini-api/docs/models/gemini
#   gemini-1.5-pro
#   gemini-1.5-pro-2
#   LearnLM
#   gemini-1.5-flash
#   gemini-1.5-flash-8b
#   aqa
#   text-embedding-004